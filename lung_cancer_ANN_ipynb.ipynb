{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcp7xNhCpSak8tFCFmT3L0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRkavin/AL-ML-/blob/main/lung_cancer_ANN_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MjrRi4d-MEuY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "VMD8MSh0MjFB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "1x2FfwF1Mqo4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean squared error loss and its derivative\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()"
      ],
      "metadata": {
        "id": "Kf6DTaNaMuTd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss_derivative(y_true, y_pred):\n",
        "    return -2 * (y_true - y_pred)\n"
      ],
      "metadata": {
        "id": "mexBYaoUMxjX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.weights_hidden = np.random.randn(input_dim, hidden_dim)\n",
        "        self.bias_hidden = np.zeros((1, hidden_dim))\n",
        "        self.weights_output = np.random.randn(hidden_dim, output_dim)\n",
        "        self.bias_output = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer_input = np.dot(X, self.weights_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y_true, y_pred, learning_rate):\n",
        "        output_error = mse_loss_derivative(y_true, y_pred) * sigmoid_derivative(y_pred)\n",
        "        hidden_error = np.dot(output_error, self.weights_output.T) * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        delta_output = np.dot(self.hidden_layer_output.T, output_error)\n",
        "        delta_hidden = np.dot(X.T, hidden_error)\n",
        "\n",
        "        self.weights_output -= learning_rate * delta_output\n",
        "        self.bias_output -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
        "        self.weights_hidden -= learning_rate * delta_hidden\n",
        "        self.bias_hidden -= learning_rate * np.sum(hidden_error, axis=0, keepdims=True)\n",
        "\n",
        "    def train(self, X, y_true, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = mse_loss(y_true, y_pred)\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "            self.backward(X, y_true, y_pred, learning_rate)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.round(self.forward(X))"
      ],
      "metadata": {
        "id": "ifu-qOLAM1JY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target.reshape(-1, 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "V8e7UHJUM9Mi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the neural network\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 5\n",
        "output_dim = 1\n",
        "epochs = 1000\n",
        "learning_rate = 0.01\n"
      ],
      "metadata": {
        "id": "tdC2RFOvNA4O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "nn.train(X_train, y_train, epochs, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIH-lGj5NFBw",
        "outputId": "ccd38c55-8de1-4562-f702-99b02c7a87bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.21551686685815663\n",
            "Epoch 10, Loss: 0.05597552418317584\n",
            "Epoch 20, Loss: 0.04043105935143633\n",
            "Epoch 30, Loss: 0.031107132420055358\n",
            "Epoch 40, Loss: 0.024607816875275128\n",
            "Epoch 50, Loss: 0.02170132631360665\n",
            "Epoch 60, Loss: 0.019878755267647544\n",
            "Epoch 70, Loss: 0.018608468122670304\n",
            "Epoch 80, Loss: 0.01765400326717075\n",
            "Epoch 90, Loss: 0.016901737373887917\n",
            "Epoch 100, Loss: 0.016283344371025295\n",
            "Epoch 110, Loss: 0.015756946071467527\n",
            "Epoch 120, Loss: 0.015296188818932306\n",
            "Epoch 130, Loss: 0.01488385586847296\n",
            "Epoch 140, Loss: 0.014508285639783848\n",
            "Epoch 150, Loss: 0.014161291998425569\n",
            "Epoch 160, Loss: 0.013836896808269128\n",
            "Epoch 170, Loss: 0.013530521875600958\n",
            "Epoch 180, Loss: 0.013238469762073532\n",
            "Epoch 190, Loss: 0.012957666054011469\n",
            "Epoch 200, Loss: 0.012685777404528269\n",
            "Epoch 210, Loss: 0.01242176153616876\n",
            "Epoch 220, Loss: 0.012166197622218138\n",
            "Epoch 230, Loss: 0.011920248632285137\n",
            "Epoch 240, Loss: 0.011684022031176631\n",
            "Epoch 250, Loss: 0.011456381786339438\n",
            "Epoch 260, Loss: 0.011235910877410597\n",
            "Epoch 270, Loss: 0.01102164788501256\n",
            "Epoch 280, Loss: 0.01081329076907937\n",
            "Epoch 290, Loss: 0.01061113589938348\n",
            "Epoch 300, Loss: 0.010415919245102467\n",
            "Epoch 310, Loss: 0.01022857726430046\n",
            "Epoch 320, Loss: 0.01004993879459843\n",
            "Epoch 330, Loss: 0.009880423401050668\n",
            "Epoch 340, Loss: 0.009719875697274567\n",
            "Epoch 350, Loss: 0.009567627665445172\n",
            "Epoch 360, Loss: 0.009422733010057844\n",
            "Epoch 370, Loss: 0.009284222494348628\n",
            "Epoch 380, Loss: 0.009151276906616875\n",
            "Epoch 390, Loss: 0.009023296538966137\n",
            "Epoch 400, Loss: 0.008899890471095505\n",
            "Epoch 410, Loss: 0.008780823842856444\n",
            "Epoch 420, Loss: 0.008665957015902056\n",
            "Epoch 430, Loss: 0.008555195769901736\n",
            "Epoch 440, Loss: 0.00844845795022763\n",
            "Epoch 450, Loss: 0.008345654627417303\n",
            "Epoch 460, Loss: 0.008246681663745113\n",
            "Epoch 470, Loss: 0.008151417853144204\n",
            "Epoch 480, Loss: 0.008059726748162138\n",
            "Epoch 490, Loss: 0.007971460212977226\n",
            "Epoch 500, Loss: 0.00788646246777279\n",
            "Epoch 510, Loss: 0.00780457391331326\n",
            "Epoch 520, Loss: 0.007725634381221168\n",
            "Epoch 530, Loss: 0.007649485683956621\n",
            "Epoch 540, Loss: 0.0075759734731572015\n",
            "Epoch 550, Loss: 0.007504948484464102\n",
            "Epoch 560, Loss: 0.007436267274018452\n",
            "Epoch 570, Loss: 0.007369792553540811\n",
            "Epoch 580, Loss: 0.007305393219296064\n",
            "Epoch 590, Loss: 0.0072429441530957595\n",
            "Epoch 600, Loss: 0.007182325855417441\n",
            "Epoch 610, Loss: 0.007123423954106071\n",
            "Epoch 620, Loss: 0.007066128617895929\n",
            "Epoch 630, Loss: 0.007010333892228578\n",
            "Epoch 640, Loss: 0.006955936965177156\n",
            "Epoch 650, Loss: 0.006902837363173629\n",
            "Epoch 660, Loss: 0.006850936069095392\n",
            "Epoch 670, Loss: 0.0068001345485653635\n",
            "Epoch 680, Loss: 0.00675033366359615\n",
            "Epoch 690, Loss: 0.006701432445600872\n",
            "Epoch 700, Loss: 0.006653326692068831\n",
            "Epoch 710, Loss: 0.006605907342839151\n",
            "Epoch 720, Loss: 0.006559058583248842\n",
            "Epoch 730, Loss: 0.006512655613522908\n",
            "Epoch 740, Loss: 0.006466562018937135\n",
            "Epoch 750, Loss: 0.006420626678181669\n",
            "Epoch 760, Loss: 0.0063746801668170235\n",
            "Epoch 770, Loss: 0.006328530664785437\n",
            "Epoch 780, Loss: 0.006281959489632611\n",
            "Epoch 790, Loss: 0.006234716597083884\n",
            "Epoch 800, Loss: 0.006186516790548322\n",
            "Epoch 810, Loss: 0.00613703806149076\n",
            "Epoch 820, Loss: 0.006085924550319145\n",
            "Epoch 830, Loss: 0.006032798099082335\n",
            "Epoch 840, Loss: 0.005977283982042819\n",
            "Epoch 850, Loss: 0.0059190571199737325\n",
            "Epoch 860, Loss: 0.005857912607309142\n",
            "Epoch 870, Loss: 0.005793855328755318\n",
            "Epoch 880, Loss: 0.005727186129667261\n",
            "Epoch 890, Loss: 0.005658542894415177\n",
            "Epoch 900, Loss: 0.00558885291974448\n",
            "Epoch 910, Loss: 0.005519186900362466\n",
            "Epoch 920, Loss: 0.00545056285168677\n",
            "Epoch 930, Loss: 0.005383782038484807\n",
            "Epoch 940, Loss: 0.0053193545885238174\n",
            "Epoch 950, Loss: 0.005257515278672395\n",
            "Epoch 960, Loss: 0.005198290840132754\n",
            "Epoch 970, Loss: 0.005141577527744014\n",
            "Epoch 980, Loss: 0.005087205027208219\n",
            "Epoch 990, Loss: 0.005034979891442753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred = nn.predict(X_test)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de7Ig2SkNIQ-",
        "outputId": "e38c1c39-7306-4e9d-9de8-9276023057b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.37%\n"
          ]
        }
      ]
    }
  ]
}